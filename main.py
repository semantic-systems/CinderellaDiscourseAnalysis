# from frame_semantic_transformer import FrameSemanticTransformerfrom gutenberg.acquire import load_etextfrom gutenberg.cleanup import strip_headers# from cleantext import clean# import nltk# from paddlenlp import Taskflow# from pprint import pprint# from fastcoref import spacy_component# import spacyfrom sentence_transformers import SentenceTransformer, utilfrom pyvis.network import Network# import networkx as nxfrom narrative import CinderellaNarrativedef get_cinderella(book_id: int):    text = strip_headers(        load_etext(book_id,                   mirror="http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/")    ).strip()    return textclass Cinderella(object):    def __init__(self, uid: int):        self.uid = uiddef toy_temporality_prototype():    cinderella_1 = CinderellaNarrative(path="narratives/cinderella/20723.txt")    cinderella_2 = CinderellaNarrative(path="narratives/cinderella/10830.txt")    cinderella_3 = CinderellaNarrative(path="narratives/cinderella/23303.txt")    print(f"Cinderella 1 sentence count: {cinderella_1.sentence_count}")    print(f"Cinderella 2 sentence count: {cinderella_2.sentence_count}")    print(f"Cinderella 3 sentence count: {cinderella_3.sentence_count}")    g = Network(height="1100px", width="100%", bgcolor="#FAEBD7", font_color="#3D2B1F", notebook=True)    g.add_nodes(list(range(cinderella_1.sentence_count)),                size=[50]*cinderella_1.sentence_count,                title=cinderella_1.segmented_text,                label=[str(index) for index in list(range(cinderella_1.sentence_count))],                color=["#F4C2C2"]*cinderella_1.sentence_count                )    start = cinderella_1.sentence_count    g.add_nodes(list(range(start, start+cinderella_2.sentence_count)),                size=[50]*cinderella_2.sentence_count,                title=cinderella_2.segmented_text,                label=[str(index) for index in list(range(cinderella_2.sentence_count))],                color=["#89CFF0"] * cinderella_2.sentence_count                )    start = cinderella_1.sentence_count+cinderella_2.sentence_count    g.add_nodes(list(range(start, start+cinderella_3.sentence_count)),                size=[50]*cinderella_3.sentence_count,                title=cinderella_3.segmented_text,                label=[str(index) for index in list(range(cinderella_3.sentence_count))],                color=["#E9D66B"] * cinderella_3.sentence_count                )    embedder = SentenceTransformer('all-MiniLM-L6-v2')    cinderella_1_embeddings = embedder.encode(cinderella_1.segmented_text, convert_to_tensor=True)    cinderella_2_embeddings = embedder.encode(cinderella_2.segmented_text, convert_to_tensor=True)    cinderella_3_embeddings = embedder.encode(cinderella_3.segmented_text, convert_to_tensor=True)    cinderella_1_index = []    cinderella_2_index = []    cinderella_3_index = []    similarity_1 = []    similarity_2 = []    for i, query in enumerate(cinderella_1_embeddings):        top_hit = util.semantic_search(query, cinderella_2_embeddings, top_k=1)[0][0]        top_hit_2 = util.semantic_search(query, cinderella_3_embeddings, top_k=1)[0][0]        cinderella_1_index.append(i)        cinderella_2_index.append(top_hit['corpus_id'])        cinderella_3_index.append(top_hit_2['corpus_id'])        similarity_1.append(top_hit['score'])        similarity_2.append(top_hit_2['score'])    print(f"cinderella_1_index {cinderella_1_index}")    print(f"cinderella_2_index {cinderella_2_index}")    # Get a list of nodes    node_list = g.get_nodes()    # Print the list of nodes    print(node_list)    for i in range(len(cinderella_1_index)):        g.add_edge(cinderella_1_index[i], cinderella_1.sentence_count+cinderella_2_index[i],                   value=similarity_1[i], color="#ED872D", size=similarity_1[i],                   title=similarity_1[i], label=similarity_1[i])    for i in range(len(cinderella_1_index)):        g.add_edge(cinderella_1.sentence_count+cinderella_2_index[i],                   cinderella_1.sentence_count+cinderella_2_index[i]+cinderella_3_index[i],                   value=similarity_2[i], color="#ED872D", size=similarity_2[i],                   title=similarity_2[i], label=similarity_2[i])    g.barnes_hut(gravity=-200, central_gravity=0, spring_length=1000, spring_strength=0.001, damping=0.09, overlap=0)    for i in range(cinderella_1.sentence_count):        g.nodes[i]["x"] = i * 150        g.nodes[i]["y"] = 0    for i in range(cinderella_1.sentence_count, cinderella_1.sentence_count+cinderella_2.sentence_count):        g.nodes[i]["x"] = (i-cinderella_1.sentence_count) * 150        g.nodes[i]["y"] = -1500    start = cinderella_1.sentence_count+cinderella_2.sentence_count    for i in range(start, start+cinderella_3.sentence_count):        g.nodes[i]["x"] = (i-1.5*cinderella_1.sentence_count) * 150        g.nodes[i]["y"] = -3000    g.toggle_physics(False)    g.show("cinderella.html")if __name__ == "__main__":    toy_temporality_prototype()    # from pyvis.network import Network    #    # # Create an empty network    # net = Network(height="800px", width="100%", bgcolor="#222222", font_color="white",notebook=True)    #    # # Add nodes with empty attributes    # nodes = list(range(10))  # Example list of nodes from 0 to 9    # for node in nodes:    #     net.add_node(node)    #    # # Define the two lists indicating the connected nodes    # list1 = [0, 2, 4, 6, 8]  # Example list of indices    # list2 = [1, 3, 5, 7, 9]  # Example list of indices    #    # # Create edges between the connected nodes with weights    # for i in range(len(list1)):    #     net.add_edge(list1[i], list2[i], value=1, color="blue")    #    # # Set the layout to 'directed' for horizontal alignment    # net.barnes_hut(gravity=-200, central_gravity=0, spring_length=100, spring_strength=0.001, damping=0.09, overlap=0)    #    # # Manually position nodes in two separate lines    # for i, node in enumerate(list1):    #     net.nodes[node]["x"] = i * 100    #     net.nodes[node]["y"] = 0    #    # for i, node in enumerate(list2):    #     net.nodes[node]["x"] = i * 100    #     net.nodes[node]["y"] = -100    #    # # Generate the HTML file for visualization    # net.show("network.html")    # frame_transformer = FrameSemanticTransformer()    # nlp = spacy.load("en_core_web_sm")    # nlp.add_pipe("fastcoref", config={'model_architecture': 'LingMessCoref', 'model_path': 'biu-nlp/lingmess-coref', 'device': 'cpu'})    #    # text = get_cinderella()    # print(text)    # clean_text = clean(text, fix_unicode=True, to_ascii=True, lower=False, no_line_breaks=True, no_urls=True, no_emails=True, lang="en")    #    # doc = nlp(  # for multiple texts use nlp.pipe    #     clean_text,    #     component_cfg={"fastcoref": {'resolve_text': True}}    # )    #    # resolved_text = doc._.resolved_text    # print(resolved_text)    # segmented_text = nltk.sent_tokenize(resolved_text)    #    # news = ["A preliminary 6.20 magnitude #earthquake has occurred near Taft, Eastern Visayas, #Philippines.",    #         "A shooting has been reported at Saugus High School in Santa Clarita just north of Los Angeles.",    #         "One person was missing following a large explosion at an apparent industrial building in Houston Friday. The blast damaged nearby buildings and homes.",    #         "Gunman murdered travelers in Hamburg train station last night.",    #         "It is so hot that I feel the earth is shaking",    #         "Earthquake: A preliminary 6.20 magnitude #earthquake has occurred near Taft, Eastern Visayas, #Philippines.",    #         "Breaking news - Earthquake event: A preliminary 6.20 magnitude #earthquake has occurred near Taft, Eastern Visayas, #Philippines.",    #         "Earthquake Event: A preliminary 6.20 magnitude #earthquake has occurred near Taft, Eastern Visayas, #Philippines.",    #         "This event describes an Earthquake: A preliminary 6.20 magnitude #earthquake has occurred near Taft, Eastern Visayas, #Philippines."]    #    # print("Information Extraction")    # ie_schema = ['Character', 'Person', "Time", "Action", "Feeling"]    # ie = Taskflow('information_extraction', schema=ie_schema, model='uie-base-en')    #cinderella    # for text in segmented_text[2:12]:    #     result = ie(text)    #     pprint(result)    # for n in news:    #     result = ie(n)    #     pprint(result)    # print("Relation Extraction")    # re_schema = [{'Person': ['Location', 'Time', "Action", "Feeling"]}, {'Conflict': ['Person', 'Location', "Reason"]}]    # re = Taskflow('information_extraction', schema=re_schema, model='uie-base-en')    #    # # cinderella    # for text in segmented_text[2:12]:    #     result = re(text)    #     pprint(result)    # for n in news:    #     result = re(n)    #     pprint(result)    # print("People Extraction")    # ee_schema = [{'Person': ["With", "In", "Feels", "Age"]}]    # ee = Taskflow('information_extraction', schema=ee_schema, model='uie-base-en')    #    # # cinderella    # for text in segmented_text[2:12]:    #     result = ee(text)    #     pprint(result)    # for n in news:    #     result = ee(n)    #     pprint(result)    # print("Narrative element Classification")    # cls_schema = ['Introduction to characters and setting', 'Events leading up to the main problem or conflict', "The problem reaches a high point", "The characters work to solve the problem or conflict", "Resolution of problems or conflicts"]    # cls = Taskflow("zero_shot_text_classification", schema=cls_schema)    #    # # cinderella    # for text in segmented_text[2:]:    #     result = cls(text)    #     pprint(result)    # for n in news:    #     result = cls(n)    #     pprint(result)    # for text in segmented_text[2:12]:        # result = frame_transformer.detect_frames(text)        #        # print(f"Results found in: {result.sentence}")        # for frame in result.frames:        #     print(f"FRAME: {frame.name}")        #     for element in frame.frame_elements:        #         print(f"{element.name}: {element.text}")        # result = ie(text)        # pprint(result)